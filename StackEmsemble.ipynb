{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"StaclEmsemble.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"mzgGq7XY6FUN","colab_type":"text"},"cell_type":"markdown","source":["# Import Libraries"]},{"metadata":{"id":"B5FplKut6FUP","colab_type":"code","outputId":"884f516f-f3f3-49c1-a635-16b5f8a91949","executionInfo":{"status":"ok","timestamp":1549894822990,"user_tz":-420,"elapsed":1105,"user":{"displayName":"Krittiya Champangta","photoUrl":"https://lh4.googleusercontent.com/-mOOtSQJZgYE/AAAAAAAAAAI/AAAAAAAAACc/HRdPIc5RVHA/s64/photo.jpg","userId":"04670264686861320734"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["from mlxtend.regressor import StackingRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.svm import SVR\n","  \n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","\n","import numpy as np\n","import pandas as pd\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"FthizzXNcqNx","colab_type":"code","colab":{}},"cell_type":"code","source":["def smape(A, F):\n","    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"d4xZpBx_6FUX","colab_type":"text"},"cell_type":"markdown","source":["# Data Preparation"]},{"metadata":{"id":"jNosP59i6FUY","colab_type":"code","outputId":"bb9a9e75-780e-4069-ccef-b854f58ccac0","executionInfo":{"status":"ok","timestamp":1549894911339,"user_tz":-420,"elapsed":89419,"user":{"displayName":"Krittiya Champangta","photoUrl":"https://lh4.googleusercontent.com/-mOOtSQJZgYE/AAAAAAAAAAI/AAAAAAAAACc/HRdPIc5RVHA/s64/photo.jpg","userId":"04670264686861320734"}},"colab":{"base_uri":"https://localhost:8080/","height":2016}},"cell_type":"code","source":["for Fold in range (10):\n","  df_train = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/Training & Testing/Nakhon/10 Folds/train_nakhon_dist_combined_mavg4_F%d.csv' %(Fold+1))\n","  df_test = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/Training & Testing/Nakhon/10 Folds/test_nakhon_dist_combined_mavg4_F%d.csv' %(Fold+1))\n","  head = df_test.iloc[:,1:4]\n","\n","  for i in range (6):\n","    attr_train = df_train.iloc[:,13:14+i]\n","    last_train = df_train.iloc[:,[20,21]]\n","    X_train = pd.concat([attr_train,last_train], axis=1)\n","    y_train = df_train.iloc[:,12]\n","    \n","    attr_test = df_test.iloc[:,13:14+i]\n","    last_test = df_test.iloc[:,[20,21]]\n","    X_test = pd.concat([attr_test,last_test], axis=1)\n","    y_test = df_test.iloc[:,12]\n","    \n","    #X_train = df_train.iloc[:,13+i:22]\n","    #y_train = df_train.iloc[:,12]\n","    #X_test = df_test.iloc[:,13+i:22]\n","    #y_test = df_test.iloc[:,12]\n","\n","    rf = RandomForestRegressor(max_depth=30, random_state=0, n_estimators=10)\n","    svr_lin = SVR(kernel='linear')\n","    #svr_rbf = SVR(kernel='rbf')\n","\n","    stregr = StackingRegressor(regressors=[rf, rf], meta_regressor=svr_lin)\n","\n","    ensembles = stregr.fit(X_train, y_train) \n","    y_hat = ensembles.predict(X_test)\n","\n","    y = pd.DataFrame(y_hat, columns = ['predicted'])\n","    file = pd.concat([head, y_test,y], axis = 1)\n","    file.columns = [['addrcode','Week','Year','actual','predicted']]\n","    file.to_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/District/NST_Dist_RF_withoutCD_DF%d_Fold%d.csv'%(i+1, Fold+1), encoding = 'utf-8')\n","    print(\"Fold%d DF%d without\"%(Fold+1, i+1))\n","\n","    ################################ WITH CD ###################################\n","    \n","for Fold in range (10):\n","  df_train = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/Training & Testing/Nakhon/10 Folds/train_nakhon_dist_combined_mavg4_F%d.csv' %(Fold+1))\n","  df_test = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/Training & Testing/Nakhon/10 Folds/test_nakhon_dist_combined_mavg4_F%d.csv' %(Fold+1))\n","  head = df_test.iloc[:,1:4]\n","  \n","  for i in range (6):\n","    attr_train = df_train.iloc[:,13:14+i]\n","    last_train = df_train.iloc[:,20:31]\n","    X_train = pd.concat([attr_train,last_train], axis=1)\n","    y_train = df_train.iloc[:,12]\n","  \n","    attr_test = df_test.iloc[:,13:14+i]\n","    last_test = df_test.iloc[:,20:31]\n","    X_test = pd.concat([attr_test,last_test], axis=1)\n","    y_test = df_test.iloc[:,12]\n","\n","    rf = RandomForestRegressor(max_depth=30, random_state=0, n_estimators=10)\n","    svr_lin = SVR(kernel='linear')\n","    svr_rbf = SVR(kernel='rbf')\n","\n","    stregr = StackingRegressor(regressors=[rf, rf], meta_regressor=svr_lin)\n","\n","    ensembles = stregr.fit(X_train, y_train)\n","    y_hat = ensembles.predict(X_test)\n","\n","    y = pd.DataFrame(y_hat, columns = ['predicted'])\n","    file = pd.concat([head, y_test,y], axis = 1)\n","    file.columns = [['addrcode','Week','Year','actual','predicted']]\n","    file.to_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/District/NST_Dist_RF_withCD_DF%d_Fold%d.csv'%(i+1, Fold+1), encoding = 'utf-8')\n","    print(\"Fold%d DF%d with\"%(Fold+1, i+1))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Fold1 DF1 without\n","Fold1 DF2 without\n","Fold1 DF3 without\n","Fold1 DF4 without\n","Fold1 DF5 without\n","Fold1 DF6 without\n","Fold2 DF1 without\n","Fold2 DF2 without\n","Fold2 DF3 without\n","Fold2 DF4 without\n","Fold2 DF5 without\n","Fold2 DF6 without\n","Fold3 DF1 without\n","Fold3 DF2 without\n","Fold3 DF3 without\n","Fold3 DF4 without\n","Fold3 DF5 without\n","Fold3 DF6 without\n","Fold4 DF1 without\n","Fold4 DF2 without\n","Fold4 DF3 without\n","Fold4 DF4 without\n","Fold4 DF5 without\n","Fold4 DF6 without\n","Fold5 DF1 without\n","Fold5 DF2 without\n","Fold5 DF3 without\n","Fold5 DF4 without\n","Fold5 DF5 without\n","Fold5 DF6 without\n","Fold6 DF1 without\n","Fold6 DF2 without\n","Fold6 DF3 without\n","Fold6 DF4 without\n","Fold6 DF5 without\n","Fold6 DF6 without\n","Fold7 DF1 without\n","Fold7 DF2 without\n","Fold7 DF3 without\n","Fold7 DF4 without\n","Fold7 DF5 without\n","Fold7 DF6 without\n","Fold8 DF1 without\n","Fold8 DF2 without\n","Fold8 DF3 without\n","Fold8 DF4 without\n","Fold8 DF5 without\n","Fold8 DF6 without\n","Fold9 DF1 without\n","Fold9 DF2 without\n","Fold9 DF3 without\n","Fold9 DF4 without\n","Fold9 DF5 without\n","Fold9 DF6 without\n","Fold10 DF1 without\n","Fold10 DF2 without\n","Fold10 DF3 without\n","Fold10 DF4 without\n","Fold10 DF5 without\n","Fold10 DF6 without\n","Fold1 DF1 with\n","Fold1 DF2 with\n","Fold1 DF3 with\n","Fold1 DF4 with\n","Fold1 DF5 with\n","Fold1 DF6 with\n","Fold2 DF1 with\n","Fold2 DF2 with\n","Fold2 DF3 with\n","Fold2 DF4 with\n","Fold2 DF5 with\n","Fold2 DF6 with\n","Fold3 DF1 with\n","Fold3 DF2 with\n","Fold3 DF3 with\n","Fold3 DF4 with\n","Fold3 DF5 with\n","Fold3 DF6 with\n","Fold4 DF1 with\n","Fold4 DF2 with\n","Fold4 DF3 with\n","Fold4 DF4 with\n","Fold4 DF5 with\n","Fold4 DF6 with\n","Fold5 DF1 with\n","Fold5 DF2 with\n","Fold5 DF3 with\n","Fold5 DF4 with\n","Fold5 DF5 with\n","Fold5 DF6 with\n","Fold6 DF1 with\n","Fold6 DF2 with\n","Fold6 DF3 with\n","Fold6 DF4 with\n","Fold6 DF5 with\n","Fold6 DF6 with\n","Fold7 DF1 with\n","Fold7 DF2 with\n","Fold7 DF3 with\n","Fold7 DF4 with\n","Fold7 DF5 with\n","Fold7 DF6 with\n","Fold8 DF1 with\n","Fold8 DF2 with\n","Fold8 DF3 with\n","Fold8 DF4 with\n","Fold8 DF5 with\n","Fold8 DF6 with\n","Fold9 DF1 with\n","Fold9 DF2 with\n","Fold9 DF3 with\n","Fold9 DF4 with\n","Fold9 DF5 with\n","Fold9 DF6 with\n","Fold10 DF1 with\n","Fold10 DF2 with\n","Fold10 DF3 with\n","Fold10 DF4 with\n","Fold10 DF5 with\n","Fold10 DF6 with\n"],"name":"stdout"}]},{"metadata":{"id":"6VhxJxh3bqmC","colab_type":"text"},"cell_type":"markdown","source":["### **RF**"]},{"metadata":{"id":"DXWwD-GRbwJG","colab_type":"code","colab":{}},"cell_type":"code","source":["im_mae=[]\n","im_rmse=[]\n","im_smape=[]\n","\n","avr_smape=[]\n","avr_smape_norm=[]\n","\n","avr_rmse=[]\n","avr_rmse_norm=[]\n","\n","avr_mae=[]\n","avr_mae_norm=[]\n","\n","addr = []\n","  \n","for i in range (6):\n","  add = i+1\n","  addr.append(add)\n","  \n","  rmse_a = []\n","  rmse_n = []\n","  mae_a =[]\n","  mae_n = []\n","  smape_a = []\n","  smape_n = []\n","\n","    \n","  for Fold in range (10):\n","    #ADJUSTED CD = WITHOUT CD\n","    withoutCD = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/District/NST_Dist_RF_withoutCD_DF%d_Fold%d.csv'%(i+1, Fold+1))\n","    predicted = withoutCD['predicted']\n","    actual = withoutCD['actual']\n","      \n","    # calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(predicted, actual))\n","    rmse_a.append(rmse)\n","    \n","    # calculate MAE\n","    mae = mean_absolute_error(predicted,actual)\n","    mae_a.append(mae)\n","        \n","    Smape = smape(predicted, actual)\n","    smape_a.append(Smape)\n","      \n","      \n","    #NORMAL CD = WITH CD\n","    withCD = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/District/NST_Dist_RF_withCD_DF%d_Fold%d.csv'%(i+1, Fold+1)) \n","    predicted_norm = withCD['predicted']\n","    actual_norm = withCD['actual']\n","      \n","    # calculate RMSE\n","    lin_mse = mean_squared_error(predicted_norm,actual_norm)\n","    rmse_norm = np.sqrt(lin_mse)\n","    rmse_n.append(rmse_norm)\n","      \n","    # calculate MAE\n","    mae_norm = mean_absolute_error(predicted_norm,actual_norm)\n","    mae_n.append(mae_norm)\n","     \n","    Smape_norm = smape(predicted_norm,actual_norm)\n","    smape_n.append(Smape_norm)\n","    \n","\n","  #%improvement\n","  #%SMAPE\n","  AVR_smape = sum(smape_a)/float(len(smape_a))\n","  avr_smape.append(AVR_smape)\n","  AVR_smape_norm = sum(smape_n)/float(len(smape_n))\n","  avr_smape_norm.append(AVR_smape_norm)\n","  \n","  improve_smape = (AVR_smape - AVR_smape_norm)/AVR_smape\n","  im_smape.append(improve_smape)\n","  #print(im_smape)\n","  \n","  #%RMSE\n","  AVR_rmse = sum(rmse_a)/float(len(rmse_a))\n","  avr_rmse.append(AVR_rmse)\n","  AVR_rmse_norm = sum(rmse_n)/float(len(rmse_n))\n","  avr_rmse_norm.append(AVR_rmse_norm)\n","  \n","  improve_rmse = (AVR_rmse - AVR_rmse_norm)/AVR_rmse\n","  im_rmse.append(improve_rmse)\n","  #print(im_rmse)\n","  \n","  #%MAE\n","  AVR_mae = sum(mae_a)/float(len(mae_a))\n","  avr_mae.append(AVR_mae)\n","  AVR_mae_norm = sum(mae_n)/float(len(mae_n))\n","  avr_mae_norm.append(AVR_mae_norm)\n","  \n","  improve_mae = (AVR_mae - AVR_mae_norm)/AVR_mae\n","  im_mae.append(improve_mae)\n","  #print(im_rmse)\n","    \n","code = pd.DataFrame({'addrcode':addr})\n","\n","smape_ad = pd.DataFrame({'smape_wo':avr_smape})\n","smape_norm = pd.DataFrame({'smape_w':avr_smape_norm})\n","   \n","rmse_ad = pd.DataFrame({'rmse_wo': avr_rmse})\n","rmse_norm = pd.DataFrame({'rmse_w':avr_rmse_norm})\n","    \n","mae_ad = pd.DataFrame({'mae_wo':avr_mae})\n","mae_norm = pd.DataFrame({'mae_w':avr_mae_norm})\n","    \n","percent_smape = pd.DataFrame({'ims':im_smape}) \n","percent_rmse = pd.DataFrame({'imr':im_rmse}) \n","percent_mae = pd.DataFrame({'imm':im_mae}) \n","    \n","file = pd.concat([code, smape_ad, smape_norm,percent_smape, rmse_ad,rmse_norm,percent_rmse,mae_ad,mae_norm, percent_mae], axis=1)\n","file.columns = [['Week ahead','SMAPE without CD', 'SMAPE with CD','% improve SMAPE','RMSE without CD','RMSE with CD','% improvve RMSE','MAE without CD','MAE with CD','% improve MAE']]\n","file.to_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/District/NST_Dist_RF_Eva.csv', encoding = 'utf-8', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4LJVCH7z9NDw","colab_type":"text"},"cell_type":"markdown","source":["## **Sub district**"]},{"metadata":{"id":"t1qKIRfZDdp_","colab_type":"code","outputId":"645fe49d-042c-4da4-e95d-00a0fe29dc0b","executionInfo":{"status":"ok","timestamp":1549896919843,"user_tz":-420,"elapsed":2097863,"user":{"displayName":"Krittiya Champangta","photoUrl":"https://lh4.googleusercontent.com/-mOOtSQJZgYE/AAAAAAAAAAI/AAAAAAAAACc/HRdPIc5RVHA/s64/photo.jpg","userId":"04670264686861320734"}},"colab":{"base_uri":"https://localhost:8080/","height":2016}},"cell_type":"code","source":["for Fold in range (10):\n","  df_train = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/Training & Testing/Nakhon/10 Folds/train_nakhon_subdist_combined_mavg4_F%d.csv' %(Fold+1))\n","  df_test = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/Training & Testing/Nakhon/10 Folds/test_nakhon_subdist_combined_mavg4_F%d.csv' %(Fold+1))\n","  head = df_test.iloc[:,1:4]\n","\n","  for i in range (6):\n","    attr_train = df_train.iloc[:,13:14+i]\n","    last_train = df_train.iloc[:,[20,21]]\n","    X_train = pd.concat([attr_train,last_train], axis=1)\n","    y_train = df_train.iloc[:,12]\n","    \n","    attr_test = df_test.iloc[:,13:14+i]\n","    last_test = df_test.iloc[:,[20,21]]\n","    X_test = pd.concat([attr_test,last_test], axis=1)\n","    y_test = df_test.iloc[:,12]\n","     \n","    #X_train = df_train.iloc[:,13+i:22]\n","    #y_train = df_train.iloc[:,12]\n","    #X_test = df_test.iloc[:,13+i:22]\n","    #y_test = df_test.iloc[:,12] \n"," \n","    rf = RandomForestRegressor(max_depth=30, random_state=0, n_estimators=10)\n","    svr_lin = SVR(kernel='linear')\n","    #svr_rbf = SVR(kernel='rbf')\n","\n","    stregr = StackingRegressor(regressors=[rf, rf], meta_regressor=svr_lin)\n","\n","    ensembles = stregr.fit(X_train, y_train)\n","    y_hat = ensembles.predict(X_test)\n","\n","    y = pd.DataFrame(y_hat, columns = ['predicted'])\n","    file = pd.concat([head, y_test,y], axis = 1)\n","    file.columns = [['addrcode','Week','Year','actual','predicted']]\n","    file.to_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/Subdistrict/NST_Subdist_RF_withoutCD_DF%d_Fold%d.csv'%(i+1, Fold+1), encoding = 'utf-8')\n","    print(\"Fold%d DF%d without\"%(Fold, i+1))\n","  \n","    ################################ WITH CD ###################################\n","    \n","for Fold in range (10):\n","  df_train = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/Training & Testing/Nakhon/10 Folds/train_nakhon_subdist_combined_mavg4_F%d.csv' %(Fold+1))\n","  df_test = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/Training & Testing/Nakhon/10 Folds/test_nakhon_subdist_combined_mavg4_F%d.csv' %(Fold+1))\n","  head = df_test.iloc[:,1:4]\n","  \n","  for i in range (6):\n","    attr_train = df_train.iloc[:,13:14+i]\n","    last_train = df_train.iloc[:,20:31]\n","    X_train = pd.concat([attr_train,last_train], axis=1)\n","    y_train = df_train.iloc[:,12]\n","  \n","    attr_test = df_test.iloc[:,13:14+i]\n","    last_test = df_test.iloc[:,20:31]\n","    X_test = pd.concat([attr_test,last_test], axis=1)\n","    y_test = df_test.iloc[:,12]\n","\n","    rf = RandomForestRegressor(max_depth=30, random_state=0, n_estimators=10)\n","    svr_lin = SVR(kernel='linear')\n","    svr_rbf = SVR(kernel='rbf')\n","\n","    stregr = StackingRegressor(regressors=[rf, rf], meta_regressor=svr_lin)\n","\n","    ensembles = stregr.fit(X_train, y_train)\n","    y_hat = ensembles.predict(X_test)\n","\n","    y = pd.DataFrame(y_hat, columns = ['predicted'])\n","    file = pd.concat([head, y_test,y], axis = 1)\n","    file.columns = [['addrcode','Week','Year','actual','predicted']]\n","    file.to_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/Subdistrict/NST_Subdist_RF_withCD_DF%d_Fold%d.csv'%(i+1, Fold+1), encoding = 'utf-8')\n","    print(\"Fold%d DF%d with\"%(Fold+1, i+1))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Fold0 DF1 without\n","Fold0 DF2 without\n","Fold0 DF3 without\n","Fold0 DF4 without\n","Fold0 DF5 without\n","Fold0 DF6 without\n","Fold1 DF1 without\n","Fold1 DF2 without\n","Fold1 DF3 without\n","Fold1 DF4 without\n","Fold1 DF5 without\n","Fold1 DF6 without\n","Fold2 DF1 without\n","Fold2 DF2 without\n","Fold2 DF3 without\n","Fold2 DF4 without\n","Fold2 DF5 without\n","Fold2 DF6 without\n","Fold3 DF1 without\n","Fold3 DF2 without\n","Fold3 DF3 without\n","Fold3 DF4 without\n","Fold3 DF5 without\n","Fold3 DF6 without\n","Fold4 DF1 without\n","Fold4 DF2 without\n","Fold4 DF3 without\n","Fold4 DF4 without\n","Fold4 DF5 without\n","Fold4 DF6 without\n","Fold5 DF1 without\n","Fold5 DF2 without\n","Fold5 DF3 without\n","Fold5 DF4 without\n","Fold5 DF5 without\n","Fold5 DF6 without\n","Fold6 DF1 without\n","Fold6 DF2 without\n","Fold6 DF3 without\n","Fold6 DF4 without\n","Fold6 DF5 without\n","Fold6 DF6 without\n","Fold7 DF1 without\n","Fold7 DF2 without\n","Fold7 DF3 without\n","Fold7 DF4 without\n","Fold7 DF5 without\n","Fold7 DF6 without\n","Fold8 DF1 without\n","Fold8 DF2 without\n","Fold8 DF3 without\n","Fold8 DF4 without\n","Fold8 DF5 without\n","Fold8 DF6 without\n","Fold9 DF1 without\n","Fold9 DF2 without\n","Fold9 DF3 without\n","Fold9 DF4 without\n","Fold9 DF5 without\n","Fold9 DF6 without\n","Fold1 DF1 with\n","Fold1 DF2 with\n","Fold1 DF3 with\n","Fold1 DF4 with\n","Fold1 DF5 with\n","Fold1 DF6 with\n","Fold2 DF1 with\n","Fold2 DF2 with\n","Fold2 DF3 with\n","Fold2 DF4 with\n","Fold2 DF5 with\n","Fold2 DF6 with\n","Fold3 DF1 with\n","Fold3 DF2 with\n","Fold3 DF3 with\n","Fold3 DF4 with\n","Fold3 DF5 with\n","Fold3 DF6 with\n","Fold4 DF1 with\n","Fold4 DF2 with\n","Fold4 DF3 with\n","Fold4 DF4 with\n","Fold4 DF5 with\n","Fold4 DF6 with\n","Fold5 DF1 with\n","Fold5 DF2 with\n","Fold5 DF3 with\n","Fold5 DF4 with\n","Fold5 DF5 with\n","Fold5 DF6 with\n","Fold6 DF1 with\n","Fold6 DF2 with\n","Fold6 DF3 with\n","Fold6 DF4 with\n","Fold6 DF5 with\n","Fold6 DF6 with\n","Fold7 DF1 with\n","Fold7 DF2 with\n","Fold7 DF3 with\n","Fold7 DF4 with\n","Fold7 DF5 with\n","Fold7 DF6 with\n","Fold8 DF1 with\n","Fold8 DF2 with\n","Fold8 DF3 with\n","Fold8 DF4 with\n","Fold8 DF5 with\n","Fold8 DF6 with\n","Fold9 DF1 with\n","Fold9 DF2 with\n","Fold9 DF3 with\n","Fold9 DF4 with\n","Fold9 DF5 with\n","Fold9 DF6 with\n","Fold10 DF1 with\n","Fold10 DF2 with\n","Fold10 DF3 with\n","Fold10 DF4 with\n","Fold10 DF5 with\n","Fold10 DF6 with\n"],"name":"stdout"}]},{"metadata":{"id":"q4Bf2Ed9db7L","colab_type":"text"},"cell_type":"markdown","source":["### **RF  Subdistrict**"]},{"metadata":{"id":"p4XfljW5ddw3","colab_type":"code","colab":{}},"cell_type":"code","source":["im_mae=[]\n","im_rmse=[]\n","im_smape=[]\n","\n","avr_smape=[]\n","avr_smape_norm=[]\n"," \n","avr_rmse=[]\n","avr_rmse_norm=[]\n","\n","avr_mae=[]\n","avr_mae_norm=[]\n","\n","addr = []\n","  \n","for i in range (6):\n","  add = i+1\n","  addr.append(add)\n","  \n","  rmse_a = []\n","  rmse_n = []\n","  mae_a =[]\n","  mae_n = []\n","  smape_a = []\n","  smape_n = []\n","\n","    \n","  for Fold in range (10):\n","    #ADJUSTED CD = WITHOUT CD\n","    withoutCD = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/Subdistrict/NST_Subdist_RF_withoutCD_DF%d_Fold%d.csv'%(i+1, Fold+1))\n","    predicted = withoutCD['predicted']\n","    actual = withoutCD['actual']\n","      \n","    # calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(predicted, actual))\n","    rmse_a.append(rmse)\n","    \n","    # calculate MAE\n","    mae = mean_absolute_error(predicted,actual)\n","    mae_a.append(mae)\n","        \n","    Smape = smape(predicted, actual)\n","    smape_a.append(Smape)\n","      \n","      \n","    #NORMAL CD = WITH CD\n","    withCD = pd.read_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/Subdistrict/NST_Subdist_RF_withCD_DF%d_Fold%d.csv'%(i+1, Fold+1)) \n","    predicted_norm = withCD['predicted']\n","    actual_norm = withCD['actual']\n","      \n","    # calculate RMSE\n","    lin_mse = mean_squared_error(predicted_norm,actual_norm)\n","    rmse_norm = np.sqrt(lin_mse)\n","    rmse_n.append(rmse_norm)\n","      \n","    # calculate MAE\n","    mae_norm = mean_absolute_error(predicted_norm,actual_norm)\n","    mae_n.append(mae_norm)\n","     \n","    Smape_norm = smape(predicted_norm,actual_norm)\n","    smape_n.append(Smape_norm)\n","    \n","\n","  #%improvement\n","  #%SMAPE\n","  AVR_smape = sum(smape_a)/float(len(smape_a))\n","  avr_smape.append(AVR_smape)\n","  AVR_smape_norm = sum(smape_n)/float(len(smape_n))\n","  avr_smape_norm.append(AVR_smape_norm)\n","  \n","  improve_smape = (AVR_smape - AVR_smape_norm)/AVR_smape\n","  im_smape.append(improve_smape)\n","  #print(im_smape)\n","  \n","  #%RMSE\n","  AVR_rmse = sum(rmse_a)/float(len(rmse_a))\n","  avr_rmse.append(AVR_rmse)\n","  AVR_rmse_norm = sum(rmse_n)/float(len(rmse_n))\n","  avr_rmse_norm.append(AVR_rmse_norm)\n","  \n","  improve_rmse = (AVR_rmse - AVR_rmse_norm)/AVR_rmse\n","  im_rmse.append(improve_rmse)\n","  #print(im_rmse)\n","  \n","  #%MAE\n","  AVR_mae = sum(mae_a)/float(len(mae_a))\n","  avr_mae.append(AVR_mae)\n","  AVR_mae_norm = sum(mae_n)/float(len(mae_n))\n","  avr_mae_norm.append(AVR_mae_norm)\n","  \n","  improve_mae = (AVR_mae - AVR_mae_norm)/AVR_mae\n","  im_mae.append(improve_mae)\n","  #print(im_rmse)\n","    \n","code = pd.DataFrame({'addrcode':addr})\n","\n","smape_ad = pd.DataFrame({'smape_wo':avr_smape})\n","smape_norm = pd.DataFrame({'smape_w':avr_smape_norm})\n","   \n","rmse_ad = pd.DataFrame({'rmse_wo': avr_rmse})\n","rmse_norm = pd.DataFrame({'rmse_w':avr_rmse_norm})\n","    \n","mae_ad = pd.DataFrame({'mae_wo':avr_mae})\n","mae_norm = pd.DataFrame({'mae_w':avr_mae_norm})\n","    \n","percent_smape = pd.DataFrame({'ims':im_smape}) \n","percent_rmse = pd.DataFrame({'imr':im_rmse}) \n","percent_mae = pd.DataFrame({'imm':im_mae}) \n","    \n","file = pd.concat([code, smape_ad, smape_norm,percent_smape, rmse_ad,rmse_norm,percent_rmse,mae_ad,mae_norm, percent_mae], axis=1)\n","file.columns = [['Week ahead','SMAPE without CD', 'SMAPE with CD','% improve SMAPE','RMSE without CD','RMSE with CD','% improvve RMSE','MAE without CD','MAE with CD','% improve MAE']]\n","file.to_csv('gdrive/My Drive/Senior Project 2018 [Dengue]/Modeling/StackEnsemble/Nakhon/10 Folds/Subdistrict/NST_Subdist_RF_Eva.csv', encoding = 'utf-8', index=False)"],"execution_count":0,"outputs":[]}]}